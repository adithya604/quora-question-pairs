{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "from nltk.corpus import wordnet\n",
    "from textblob import Word\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neighbors import NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('raw_data/train.csv').fillna('')[:1000]\n",
    "qs1, qs2, dupl = list(df['question1']), list(df['question2']), list(df['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': ['visit', 'France']}"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Token(object):\n",
    "    ''' stores information about single token'''\n",
    "    \n",
    "    def __init__(self, token):\n",
    "        if isinstance(token, str):\n",
    "            if re.match(r'_text:([\\-a-zA-Z0-9]*),?_lemma:([\\-a-zA-Z0-9]*),?_pos:([\\-a-zA-Z0-9]*),?_ent_type:([\\-a-zA-Z0-9]*),?', token) is None:\n",
    "                raise ValueError()\n",
    "            \n",
    "            self.text = re.findall(r'_text:([\\-a-zA-Z0-9]*),?', token)[0]\n",
    "            self.lemma_ = re.findall(r'_lemma:([\\-a-zA-Z0-9]*),?', token)[0]\n",
    "            self.pos_ = re.findall(r'_pos:([\\-a-zA-Z0-9]*),?', token)[0]\n",
    "            self.ent_type_ = re.findall(r'_ent_type:([\\-a-zA-Z0-9]*),?', token)[0]\n",
    "        else:\n",
    "            if token.text == ' ':\n",
    "                raise ValueError()\n",
    "            \n",
    "            self.text = token.text\n",
    "            self.lemma_ = token.lemma_\n",
    "            self.pos_ = token.pos_\n",
    "            self.ent_type_ = token.ent_type_\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.text\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.text\n",
    "    \n",
    "    def to_str(self):\n",
    "        return '_text:{},_lemma:{},_pos:{},_ent_type:{},'.format(self.text, self.lemma_, self.pos_, self.ent_type_)\n",
    "\n",
    "    \n",
    "class Tokens(object):\n",
    "    ''' stores several tokens'''\n",
    "    \n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = self.load(tokens)\n",
    "    \n",
    "    def load(self, tokens):\n",
    "        out = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                out.append(Token(token))\n",
    "            except ValueError:\n",
    "                # incorrect token, most likely 'space'\n",
    "                continue\n",
    "                \n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str([t.text for t in self.tokens])\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str([t.text for t in self.tokens])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.i < len(self.tokens):\n",
    "            out = self.tokens[self.i]\n",
    "            self.i += 1\n",
    "            return out\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def to_str(self):\n",
    "        return ' '.join([t.to_str() for t in self.tokens])\n",
    "    \n",
    "    def ext_pattern(self, pattern):\n",
    "        ''' extends regex pattern to be able to match all fields of token'''\n",
    "        \n",
    "        fields = ['_text', '_lemma', '_pos', '_ent_type']\n",
    "        for i, _ in enumerate(fields):\n",
    "            in_pattern = ''\n",
    "            out_pattern = ''\n",
    "            for j, field in enumerate(fields):\n",
    "                if i == j:\n",
    "                    in_pattern += '{}:([\\-a-zA-Z0-9]*),?'.format(field)\n",
    "                else:\n",
    "                    in_pattern += '(?:{}:([\\-a-zA-Z0-9]*),?)?'.format(field)\n",
    "                \n",
    "                out_pattern += '{}:\\{},'.format(field, j + 1)\n",
    "            \n",
    "            pattern = re.sub(\n",
    "                in_pattern, \n",
    "                out_pattern,\n",
    "                pattern\n",
    "            )\n",
    "            \n",
    "        for field in fields:\n",
    "            in_pattern = '{}:,'.format(field)\n",
    "            out_pattern = '{}:[\\-a-zA-Z0-9]*,'.format(field)\n",
    "            \n",
    "            pattern = re.sub(\n",
    "                in_pattern, \n",
    "                out_pattern,\n",
    "                pattern\n",
    "            )\n",
    "        \n",
    "        return pattern\n",
    "    \n",
    "    def match(self, pattern):\n",
    "        m = re.match(self.ext_pattern(pattern), self.to_str())\n",
    "        if m is not None:\n",
    "            return dict([(k, Tokens(v.split(' '))) for k, v in m.groupdict().items()])\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    def sub(self, pattern, repl):\n",
    "        new_str = re.sub(self.ext_pattern(pattern), repl, self.to_str())\n",
    "        return Tokens(new_str.split(' '))\n",
    "        \n",
    "\n",
    "parsed = nlp(\"What is the most fast way to visit France?\")\n",
    "tks = Tokens(parsed)\n",
    "tks.sub(r'_lemma:can _pos:PRON', '_text:to,_lemma:to,_pos:to')\n",
    "tks.match(r'_lemma:what _lemma:be (_pos:DET )?(_pos:ADV |_pos:ADJ )*(_lemma:way|_lemma:method|_lemma:source) (_lemma:to|_lemma:of) ?(?P<target>.*)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEFAULT_PATH = 'top20k-english-words.txt'\n",
    "\n",
    "class SpellingCorrector():\n",
    "    \n",
    "    MIN_LEN = 4\n",
    "    MIN_RATIO = 2\n",
    "\n",
    "    def __init__(self, whitelist_path=DEFAULT_PATH):\n",
    "        self.whitelist_path = whitelist_path\n",
    "        self.whitelist = None\n",
    "        self.tokens = {}\n",
    "        self.corrections = {}\n",
    "    \n",
    "    @staticmethod\n",
    "    def _tokenize(s):\n",
    "        return tuple(re.findall(r'\\w+|\\W+', s))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _edits1(word):\n",
    "        ''' Based on http://norvig.com/spell-correct.html'''\n",
    "        \n",
    "        letters    = 'abcdefghijklmnopqrstuvwxyz-'\n",
    "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "    \n",
    "    def _correct_token(self, old_token):        \n",
    "        if old_token in self.corrections:\n",
    "            return self.corrections[old_token]\n",
    "        \n",
    "        candidates = [(e, self.tokens.get(e, 0)) for e in list(self._edits1(old_token)) + [old_token]]\n",
    "        candidates_sorted = sorted(candidates, key=lambda x: -x[1])\n",
    "        \n",
    "        new_token = candidates_sorted[0][0]\n",
    "        new_cnt = self.tokens.get(new_token, 0)\n",
    "        old_cnt = self.tokens.get(old_token, 0)\n",
    "        \n",
    "        if len(new_token) >= self.MIN_LEN and new_cnt > self.MIN_RATIO*old_cnt and old_token not in self.whitelist:\n",
    "            correct_token = new_token\n",
    "        else:\n",
    "            correct_token = old_token\n",
    "        \n",
    "        self.corrections[old_token] = correct_token\n",
    "\n",
    "        return correct_token\n",
    "    \n",
    "    def _correct(self, jt):\n",
    "        tokens = self._tokenize(jt)\n",
    "        new_jt = ''.join([self._correct_token(t) for t in tokens])\n",
    "        return ''.join([self._correct_token(t) for t in tokens])\n",
    "    \n",
    "    def _load_whitelist(self):\n",
    "        out = set()\n",
    "        with open(self.whitelist_path, 'r') as f:\n",
    "            for line in f:\n",
    "                out.add(line.strip('\\n'))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def fit(self, data, *args):\n",
    "        for row in data:\n",
    "            tokens = self._tokenize(row)\n",
    "            for token in tokens:\n",
    "                if token not in self.tokens:\n",
    "                    self.tokens[token] = 0\n",
    "                self.tokens[token] += 1\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def transform(self, data):\n",
    "        if self.whitelist is None:\n",
    "            self.whitelist = self._load_whitelist()\n",
    "        \n",
    "        out_data = []\n",
    "        for row in data:\n",
    "            out_data.append(self._correct(row))\n",
    "            \n",
    "        return out_data\n",
    "\n",
    "\n",
    "class Parser(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, rows, *args):\n",
    "        return self\n",
    "\n",
    "    def transform(self, rows):\n",
    "        out = []\n",
    "        for s in rows:\n",
    "            raw = s\n",
    "            parsed = Tokens(nlp(s))\n",
    "            \n",
    "            out.append({\n",
    "                'raw': raw, \n",
    "                'parsed': parsed,\n",
    "                'query': None,\n",
    "            })\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class StripPunctuation(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, rows, *args):\n",
    "        return self\n",
    "\n",
    "    def transform(self, rows):\n",
    "        out = []\n",
    "        for row in rows:\n",
    "            out.append({\n",
    "                'raw': row['raw'],\n",
    "                'parsed': row['parsed'].sub(r'_pos:PUNCT', ''),\n",
    "                'query': row['query'],\n",
    "            })\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class UnifySyntax(BaseEstimator, TransformerMixin):\n",
    "    # Where can I VERB => Where to VERB\n",
    "    # How does one VERB => How to VERB\n",
    "    \n",
    "    def fit(self, rows, *args):\n",
    "        return self\n",
    "\n",
    "    def transform(self, rows):\n",
    "        out = []\n",
    "        for row in rows:\n",
    "            out.append({\n",
    "                'raw': row['raw'], \n",
    "                'parsed': row['parsed'] \\\n",
    "                    .sub(r'(_lemma:be|_lemma:do|_lemma:can|_lemma:could|_lemma:shall|_lemma:should|_lemma:will|_lemma:would) (_pos:PRON|_lemma:one)', '_text:to,_lemma:to,_pos:to,_ent_type:'),\n",
    "                'query': row['query'],\n",
    "            })\n",
    "            \n",
    "        return out\n",
    "\n",
    "    \n",
    "class GetQuery(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, rows, *args):\n",
    "        return self\n",
    "\n",
    "    def transform(self, rows):\n",
    "        out = []\n",
    "        for row in rows:\n",
    "            query = row['query']\n",
    "            if query is not None:\n",
    "                out.append(deepcopy(row))\n",
    "                continue\n",
    "                \n",
    "            for pattern in self.patterns:\n",
    "                args = row['parsed'].match(pattern)\n",
    "                if args is not None:\n",
    "                    query = {}\n",
    "                    query['args'] = args\n",
    "                    query['type'] = self.type\n",
    "                    break\n",
    "\n",
    "            out.append({\n",
    "                'raw': row['raw'], \n",
    "                'parsed': row['parsed'],\n",
    "                'query': query,\n",
    "            })\n",
    "        \n",
    "        return out \n",
    "    \n",
    "    \n",
    "class HowToQuestion(GetQuery):\n",
    "\n",
    "    def __init__(self):\n",
    "        # How to \n",
    "        # What to do to \n",
    "        # What are some ways to , What is the way to , What are the most efective method to \n",
    "        # How can \n",
    "        # What is the step by step guide\n",
    "        \n",
    "        self.type = 'HowTo'\n",
    "        self.patterns = [\n",
    "            r'_lemma:how _lemma:to ?(?P<target>.*)',\n",
    "            r'_lemma:what _lemma:to _lemma:do _lemma:to ?(?P<target>.*)',            \n",
    "            r'_lemma:what _lemma:be (_pos:DET )?(_pos:ADV |_pos:ADJ )*((_lemma:step _lemma:by _lemma:step ))?(_lemma:way|_lemma:method|_lemma:source|_lemma:guide) (_lemma:to|_lemma:of) ?(?P<target>.*)',\n",
    "            r'_lemma:how _lemma:can ?(?P<target>.*)',\n",
    "        ]\n",
    "\n",
    "class FactWhatQuestion(GetQuery):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 'What is Affiliate Marketing?'\n",
    "        # 'What are the best white tequila brands?'\n",
    "        # 'When is new years eve?'\n",
    "        # \n",
    "        \n",
    "        self.type = 'FactWhat'\n",
    "        self.patterns = [\n",
    "            r'(_lemma:what|_lemma:which) _lemma:be ?(?P<subject>.*)',\n",
    "        ]\n",
    "        \n",
    "class FactWhoQuestion(GetQuery):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 'What is Affiliate Marketing?'\n",
    "        # 'What are the best white tequila brands?'\n",
    "        # 'When is new years eve?'\n",
    "        # \n",
    "        \n",
    "        self.type = 'FactWho'\n",
    "        self.patterns = [\n",
    "            r'_lemma:who _lemma:be ?(?P<subject>.*)',\n",
    "        ]\n",
    "\n",
    "class FactWhenQuestion(GetQuery):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 'What is Affiliate Marketing?'\n",
    "        # 'What are the best white tequila brands?'\n",
    "        # 'When is new years eve?'\n",
    "        # \n",
    "        \n",
    "        self.type = 'FactWhen'\n",
    "        self.patterns = [\n",
    "            r'_lemma:when _lemma:be ?(?P<subject>.*)',\n",
    "        ]\n",
    "        \n",
    "class FactWhereQuestion(GetQuery):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 'What is Affiliate Marketing?'\n",
    "        # 'What are the best white tequila brands?'\n",
    "        # 'When is new years eve?'\n",
    "        # \n",
    "        \n",
    "        self.type = 'FactWhere'\n",
    "        self.patterns = [\n",
    "            r'_lemma:where _lemma:be ?(?P<subject>.*)',\n",
    "        ]\n",
    "        \n",
    "\n",
    "class TrueFalseQuestion(GetQuery):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 'Are ferrari cars faster than porsche?'\n",
    "        # 'Are black beans complex carbs?'\n",
    "    \n",
    "        NP1 = r'(( _pos:ADJ)*(( _pos:ADV)* _pos:VERB)?( _pos:ADJ)*( (_pos:NOUN)|(_pos:PROPN)){1,3})'\n",
    "        NP2 = r'( _pos:ADV _pos:ADP( _pos:ADJ)*( (_pos:NOUN)|(_pos:PROPN)){1,3})'\n",
    "        NP3 = r'( _pos:VERB .*)'\n",
    "        NP4 = r'( (_pos:ADJ)|(_pos:ADV))'\n",
    "        NP5 = r'(( (_pos:NOUN)|(_pos:PROPN)){1,3} _pos:VERB( (_pos:NOUN)|(_pos:PROPN)){1,3})'\n",
    "        NP = '(' + '|'.join([NP1, NP2, NP3, NP4]) + ')'\n",
    "        PP = NP + r'( _pos:ADP' + NP + ')?'\n",
    "                \n",
    "        self.type = 'TrueFalse'\n",
    "        self.patterns = [\n",
    "            r'_lemma:be(?P<subject>{})(?P<property>{})'.format(PP, PP),\n",
    "        ]\n",
    "        \n",
    "        \n",
    "class Lemmatizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, rows, *args):\n",
    "        return self\n",
    "\n",
    "    def transform(self, rows):\n",
    "        out = []\n",
    "        for row in rows:\n",
    "            out.append({\n",
    "                'raw': row['raw'], \n",
    "                'parsed': [t.lemma_ for t in row['parsed']],\n",
    "                'query': row['query'],\n",
    "            })\n",
    "            \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(rows):\n",
    "    pipeline = make_pipeline(\n",
    "        SpellingCorrector(),\n",
    "        Parser(),\n",
    "        StripPunctuation(),\n",
    "        UnifySyntax(),\n",
    "        HowToQuestion(),\n",
    "        FactWhatQuestion(),\n",
    "        FactWhoQuestion(),\n",
    "        FactWhereQuestion(),\n",
    "        FactWhenQuestion(),\n",
    "        TrueFalseQuestion(),\n",
    "#        Lemmatizer(),\n",
    "    )\n",
    "    \n",
    "    return pipeline.fit_transform(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'parsed': ['How', 'does', '3D', 'printing', 'work'],\n",
       "  'query': None,\n",
       "  'raw': 'How does 3D printing work?'}]"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize([\"How does 3D printing work?\"])#[0]['parsed'].to_str().split( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fully normalized 0: 0.003\n",
      "Fully normalized 1: 0.016\n",
      "\n",
      "Query type: 0.447\n",
      "Same type 0: 0.840\n",
      "Same type 1: 0.947\n",
      "\n",
      "CPU times: user 10.4 s, sys: 50.3 ms, total: 10.4 s\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def count_similarity(rows1, rows2, dupl):\n",
    "    def get_type(q1, q2):\n",
    "        if q1['query'] is None or q2['query'] is None:\n",
    "            return np.NaN\n",
    "        else:\n",
    "            return q1['query']['type'] == q2['query']['type']\n",
    "    \n",
    "    def get_normalized(q1, q2):\n",
    "        # todo\n",
    "        if q1['parsed'] == q2['parsed']:\n",
    "            return True\n",
    "        else:\n",
    "            if q1['query'] is None or q2['query'] is None:\n",
    "                return False\n",
    "            else:\n",
    "                return str(q1['query']['args']) == str(q2['query']['args'])\n",
    "    \n",
    "    sim0 = np.mean([get_normalized(q1, q2) for q1, q2, d in zip(rows1, rows2, dupl) if d == 0])\n",
    "    sim1 = np.mean([get_normalized(q1, q2) for q1, q2, d in zip(rows1, rows2, dupl) if d == 1])\n",
    "\n",
    "    query_all = np.mean([q['query'] is not None for q in rows1 + rows2])\n",
    "    \n",
    "    query0 = np.nanmean([get_type(q1, q2) for q1, q2, d in zip(rows1, rows2, dupl) if d == 0])\n",
    "    query1 = np.nanmean([get_type(q1, q2) for q1, q2, d in zip(rows1, rows2, dupl) if d == 1])\n",
    "    \n",
    "    return sim0, sim1, query_all, query0, query1\n",
    "\n",
    "rows1 = normalize(qs1)\n",
    "rows2 = normalize(qs2)\n",
    "print('''\n",
    "Fully normalized 0: {:.3f}\n",
    "Fully normalized 1: {:.3f}\n",
    "\n",
    "Query type: {:.3f}\n",
    "Same type 0: {:.3f}\n",
    "Same type 1: {:.3f}\n",
    "'''.format(*count_similarity(rows1, rows2, dupl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.31 s, sys: 246 ms, total: 4.56 s\n",
      "Wall time: 4.6 s\n"
     ]
    }
   ],
   "source": [
    "# wordnet unify experiments\n",
    "\n",
    "%%time\n",
    "\n",
    "parsed_synsets = {} # pos : lemmas : name\n",
    "for synset in wordnet.all_synsets():\n",
    "    pos = synset.pos()\n",
    "    name = synset.name().split('.')[0]\n",
    "    lemmas = tuple(synset.lemma_names())\n",
    "    \n",
    "    if pos not in parsed_synsets:\n",
    "        parsed_synsets[pos] = {}\n",
    "    parsed_synsets[pos][lemmas] = name\n",
    "\n",
    "\n",
    "synset_vec = {} # pos : name : vec\n",
    "for pos, synsets in parsed_synsets.items():\n",
    "    synset_vec[pos] = {}\n",
    "    \n",
    "    for lemmas, name in synsets.items():\n",
    "        tokens = []\n",
    "        for lemma in lemmas:\n",
    "            for token in lemma.split('_'):\n",
    "                tokens.append(token)\n",
    "    \n",
    "        vec = np.zeros((1, 300), dtype=np.float32)\n",
    "        for wordnet_token in tokens:\n",
    "            for spacy_token in nlp(wordnet_token):\n",
    "                spacy_vec = spacy_token.vector\n",
    "                if sum(abs(spacy_vec)) < 0.001:\n",
    "                    vec += np.random.rand(1, 300)\n",
    "                else:\n",
    "                    vec += spacy_vec\n",
    "\n",
    "        synset_vec[pos][name] = vec\n",
    "\n",
    "pos_nn = {} # pos : [names, nn]\n",
    "for pos, name_vec in synset_vec.items():\n",
    "    names = list(name_vec.keys())\n",
    "    vec_lst = list(name_vec.values())\n",
    "    vec_matrix = np.concatenate(vec_lst, axis=0)\n",
    "    \n",
    "    nn = NearestNeighbors(\n",
    "        n_neighbors=5,\n",
    "        metric='cosine',\n",
    "        algorithm='brute',\n",
    "    )\n",
    "    nn.fit(vec_matrix)\n",
    "\n",
    "    pos_nn[pos] = [names, nn]\n",
    "\n",
    "\n",
    "def get_similar(token, pos):\n",
    "    vec = np.reshape(nlp(token)[0].vector, (1, 300))\n",
    "    names, nn = pos_nn[pos]\n",
    "    kn = nn.kneighbors(vec)\n",
    "    \n",
    "    for d, i in zip(kn[0][0], kn[1][0]):\n",
    "        print(names[i], d)\n",
    "\n",
    "get_similar('gorilla', 'n'), Word('gorilla').synsets[0].lemma_names(), nlp('gorilla').vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
