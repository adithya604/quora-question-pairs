{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PiotrRybak/.pyenv/versions/3.6.1/envs/base/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import editdistance\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer, log_loss, roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(pipeline, df, y, score_func=log_loss, return_df=False):\n",
    "    ''' Split data into train and validation, fit model and print its score '''\n",
    "    \n",
    "    df_train, df_test, y_train, y_test = train_test_split(df, y, random_state=42)\n",
    "    model = pipeline.fit(df_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(df_test)\n",
    "    score_train = score_func(y_train, model.predict_proba(df_train))\n",
    "    score_test = score_func(y_test, model.predict_proba(df_test))\n",
    "    \n",
    "    print(score_train, score_test)\n",
    "    \n",
    "    if return_df:\n",
    "        return df_test[y_pred != y_test]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Levenshtein():\n",
    "    ''' Copied from: https://gist.github.com/curzona/9435822 with few minor changes '''\n",
    "\n",
    "    def get_edits(self, s1, s2, key=hash):\n",
    "        '''Calculates the levenshtein distance and the edits between two strings'''\n",
    "        \n",
    "        try:\n",
    "            rows = self.costmatrix(\n",
    "                [t.lemma_ for t in s1], \n",
    "                [t.lemma_ for t in s2], \n",
    "                key,\n",
    "            )\n",
    "        except AttributeError:\n",
    "            rows = self.costmatrix(s1, s2, key)\n",
    "            \n",
    "        return self.backtrace(s1, s2, rows)\n",
    "\n",
    "    def costmatrix(self, s1, s2, key=hash):\n",
    "        ''' Generate the cost matrix for the two strings '''\n",
    "        rows = []\n",
    "\n",
    "        previous_row = range(len(s2) + 1)\n",
    "        rows.append(list(previous_row))\n",
    "\n",
    "        for i, c1 in enumerate(s1):\n",
    "            current_row = [i + 1]\n",
    "            for j, c2 in enumerate(s2):\n",
    "                insertions = previous_row[j + 1] + 1\n",
    "                deletions = current_row[j] + 1\n",
    "                substitutions = previous_row[j] + (key(c1) != key(c2))\n",
    "                current_row.append(min(insertions, deletions, substitutions))\n",
    "\n",
    "            previous_row = current_row\n",
    "            rows.append(previous_row)\n",
    "\n",
    "        return rows\n",
    "\n",
    "    def backtrace(self, s1, s2, rows):\n",
    "        ''' Trace back through the cost matrix to generate the list of edits '''\n",
    "        i, j = len(s1), len(s2)\n",
    "        edits = []\n",
    "\n",
    "        while not (i == 0 and j == 0):\n",
    "            prev_cost = rows[i][j]\n",
    "            neighbors = []\n",
    "\n",
    "            if i != 0 and j != 0:\n",
    "                neighbors.append(rows[i-1][j-1])\n",
    "            if i != 0:\n",
    "                neighbors.append(rows[i-1][j])\n",
    "            if j != 0:\n",
    "                neighbors.append(rows[i][j-1])\n",
    "\n",
    "            min_cost = min(neighbors)\n",
    "\n",
    "            if min_cost == prev_cost:\n",
    "                i, j = i-1, j-1\n",
    "                edits.append({'type':'match', 'from':s1[i], 'to':s2[j]})\n",
    "            elif i != 0 and j != 0 and min_cost == rows[i-1][j-1]:\n",
    "                i, j = i-1, j-1\n",
    "                edits.append({'type':'substitution', 'from':s1[i], 'to':s2[j]})\n",
    "            elif i != 0 and min_cost == rows[i-1][j]:\n",
    "                i, j = i-1, j\n",
    "                edits.append({'type':'deletion', 'from':s1[i], 'to':None})\n",
    "            elif j != 0 and min_cost == rows[i][j-1]:\n",
    "                i, j = i, j-1\n",
    "                edits.append({'type':'insertion', 'from':None, 'to':s2[j]})\n",
    "\n",
    "        edits.reverse()\n",
    "\n",
    "        return edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PandasSelector(BaseEstimator, TransformerMixin):\n",
    "    ''' Selects given columns from DataFrame and converts them into array '''\n",
    "    \n",
    "    def __init__(self, selected_columns, new_col_names):\n",
    "        self.selected_columns = selected_columns\n",
    "        self.new_col_names = new_col_names\n",
    "    \n",
    "    def fit(self, df, *args):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df_new = df[self.selected_columns].copy()\n",
    "        df_new.rename(\n",
    "            columns=dict(zip(\n",
    "                self.selected_columns, \n",
    "                self.new_col_names,\n",
    "            )), \n",
    "            inplace=True,\n",
    "        )\n",
    "        return df_new\n",
    "\n",
    "    \n",
    "class Parser(BaseEstimator, TransformerMixin):\n",
    "    ''' Parse given columns with SpaCy '''\n",
    "    \n",
    "    def __init__(self, selected_columns, new_col_names):\n",
    "        self.selected_columns = selected_columns\n",
    "        self.new_col_names = new_col_names\n",
    "    \n",
    "    def fit(self, df, *args):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        out = []\n",
    "        for i, row in tqdm(df.iterrows()):\n",
    "            out_row = {}\n",
    "            for in_col, out_col in zip(self.selected_columns, self.new_col_names):\n",
    "                out_row[out_col] = nlp(row[in_col])\n",
    "            out.append(out_row)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Common(BaseEstimator, TransformerMixin):\n",
    "    ''' Base class to calculate set of statistics for common tokens of two sentences '''\n",
    "\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    stops.update(list(string.punctuation))\n",
    "    stops.add('-PRON-')\n",
    "    stops.add('be')\n",
    "    \n",
    "    def __init__(self, use_idf=False, ngram=1):\n",
    "        self.use_idf = use_idf\n",
    "        self.idf = {}\n",
    "        self.n_docs = 0\n",
    "        self.ngram = ngram\n",
    "            \n",
    "    @staticmethod\n",
    "    def filter_tokens(tokens):\n",
    "        return NotImplemented\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_ngrams(tokens, n=1):\n",
    "        out_tokens = []\n",
    "\n",
    "        n_tokens = len(tokens)\n",
    "        for i in range(n_tokens):\n",
    "            if i + n <= n_tokens:\n",
    "                out_tokens.append(tuple(tokens[i:(i+n)]))\n",
    "\n",
    "        return out_tokens\n",
    "    \n",
    "    def get_tokens(self, tokens):\n",
    "        tokens = [t for t in tokens if t.lemma_ not in self.stops]\n",
    "        tokens = self.filter_tokens(tokens)\n",
    "        tokens = self.get_ngrams(tokens, n=self.ngram)\n",
    "        return set(tokens)\n",
    "    \n",
    "    def idf_sum(self, tokens):\n",
    "        if self.use_idf:\n",
    "            return sum([np.log(self.n_docs/(1 + self.idf.get(t, 0))) for t in tokens])\n",
    "        else:\n",
    "            return len(tokens)\n",
    "    \n",
    "    def fit(self, rows, y, *args):\n",
    "        if not self.use_idf:\n",
    "            return self\n",
    "        \n",
    "        for dupl, row in zip(y, rows):\n",
    "            self.n_docs += 1\n",
    "            \n",
    "            tokens1 = self.get_tokens(row['s1'])\n",
    "            tokens2 = self.get_tokens(row['s2'])\n",
    "            \n",
    "            for token in (tokens1 | tokens2):\n",
    "                if token not in self.idf:\n",
    "                    self.idf[token] = 0\n",
    "                self.idf[token] += 1\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, rows):\n",
    "        out = []\n",
    "        for row in rows:\n",
    "            tokens1 = self.get_tokens(row['s1'])\n",
    "            tokens2 = self.get_tokens(row['s1'])\n",
    "            \n",
    "            # levenstein distance\n",
    "            dist = editdistance.eval(tuple(tokens1), tuple(tokens2))\n",
    "            \n",
    "            out.append([\n",
    "                2*self.idf_sum(tokens1 & tokens2) / (0.1 + self.idf_sum(tokens1) + self.idf_sum(tokens2)),\n",
    "                self.idf_sum(tokens1 & tokens2) / max(0.1, min(self.idf_sum(tokens1), self.idf_sum(tokens2))),\n",
    "                self.idf_sum(tokens1 & tokens2) / max(0.1, self.idf_sum(tokens1), self.idf_sum(tokens2)),\n",
    "                self.idf_sum(tokens1 & tokens2) / (0.1 + self.idf_sum(tokens1 | tokens2)),\n",
    "                dist,\n",
    "                dist / (1 + len(tokens1) + len(tokens2)),\n",
    "                dist / max(1, len(tokens1), len(tokens2)),\n",
    "            ])\n",
    "            \n",
    "        return np.array(out)\n",
    "    \n",
    "    \n",
    "class CommonEnts(Common):\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_tokens(tokens):\n",
    "        ''' Leaves only named entities '''\n",
    "        \n",
    "        return [t.lemma_ for t in tokens if t.ent_type_]\n",
    "\n",
    "    \n",
    "class CommonNouns(Common):\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_tokens(tokens):\n",
    "        ''' Leaves only nouns '''\n",
    "        \n",
    "        return [t.lemma_ for t in tokens if t.pos_ == 'NOUN']\n",
    "\n",
    "    \n",
    "class CommonVerbs(Common):\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_tokens(tokens):\n",
    "        ''' Leaves only verbs '''\n",
    "        \n",
    "        return [t.lemma_ for t in tokens if t.pos_ == 'VERB']\n",
    "\n",
    "    \n",
    "class CommonAds(Common):\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_tokens(tokens):\n",
    "        ''' Leaves only adjectives and adverbs '''\n",
    "        \n",
    "        return [t.lemma_ for t in tokens if t.pos_ == 'ADJ' or t.pos_ == 'ADV']\n",
    "\n",
    "    \n",
    "class CommonTokens(Common):\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_tokens(tokens):\n",
    "        ''' Leaves all tokens '''\n",
    "        \n",
    "        return [t.lemma_ for t in tokens]\n",
    "\n",
    "    \n",
    "class ExtractCommonTokens(Common):\n",
    "    ''' Extract common tokens from two sentences '''\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_tokens(tokens):\n",
    "        return [t.lemma_ for t in tokens]\n",
    "\n",
    "    def get_tokens(self, tokens):\n",
    "        tokens = [t for t in tokens if t.lemma_ not in self.stops]\n",
    "        tokens = self.filter_tokens(tokens)\n",
    "        return set(tokens)\n",
    "\n",
    "    def transform(self, rows):\n",
    "        out = []\n",
    "        for row in rows:\n",
    "            tokens1 = self.get_tokens(row['s1'])\n",
    "            tokens2 = self.get_tokens(row['s2'])\n",
    "            \n",
    "            out.append(' '.join(tokens1 & tokens2))\n",
    "            \n",
    "        return np.array(out)\n",
    "\n",
    "    \n",
    "class ExtractDifferenceTokens(Common):\n",
    "    ''' Extract symetric sum of tokens from two sentences '''\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_tokens(tokens):\n",
    "        return [t.lemma_ for t in tokens]\n",
    "\n",
    "    def get_tokens(self, tokens):\n",
    "        tokens = [t for t in tokens if t.lemma_ not in self.stops]\n",
    "        tokens = self.filter_tokens(tokens)\n",
    "        return set(tokens)\n",
    "\n",
    "    def transform(self, rows):\n",
    "        out = []\n",
    "        for row in rows:\n",
    "            tokens1 = self.get_tokens(row['s1'])\n",
    "            tokens2 = self.get_tokens(row['s2'])\n",
    "            \n",
    "            out.append(' '.join(tokens1 ^ tokens2))\n",
    "            \n",
    "        return np.array(out)\n",
    "    \n",
    "    \n",
    "class QuestionFreq(BaseEstimator, TransformerMixin):\n",
    "    ''' Calculates how often given question was present in train set '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.freq = {}\n",
    "    \n",
    "    @staticmethod\n",
    "    def lemmas(tokens):\n",
    "        return ' '.join([t.lemma_ for t in tokens])\n",
    "    \n",
    "    def fit(self, rows, *args):\n",
    "        for row in rows:\n",
    "            for q in [row['s1'], row['s2']]:\n",
    "                q = self.lemmas(q)\n",
    "                if q not in self.freq:\n",
    "                    self.freq[q] = 0\n",
    "                self.freq[q] += 1\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, rows):\n",
    "        out = []\n",
    "        \n",
    "        for row in rows:\n",
    "            out.append([\n",
    "                self.freq.get(self.lemmas(row['s1']), 0),\n",
    "                self.freq.get(self.lemmas(row['s2']), 0),\n",
    "            ])\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "class EstimatorToTransform(BaseEstimator, TransformerMixin):\n",
    "    ''' Converts Estimator to Transform, which allows for stacking Estimators '''\n",
    "    \n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "    \n",
    "    def fit(self, X, *args):\n",
    "        self.estimator.fit(X, *args)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        pred = self.estimator.predict_proba(X)[:, 1]\n",
    "        return pred.reshape(-1, 1)\n",
    "\n",
    "    \n",
    "class Joiner(BaseEstimator, TransformerMixin):\n",
    "    ''' Joins iterable of strings'''\n",
    "    \n",
    "    def fit(self, rows, y, *args):\n",
    "        return self\n",
    "\n",
    "    def transform(self, rows):\n",
    "        out = []\n",
    "        for row in rows:\n",
    "            out.append(' '.join(row))\n",
    "        \n",
    "        return out\n",
    "\n",
    "class Similarity(BaseEstimator, TransformerMixin):\n",
    "    ''' Base class to calculate similarity between to sentences '''\n",
    "    \n",
    "    def similarity(self, t1, t2):\n",
    "        return NotImplemented\n",
    "    \n",
    "    def calc_pairwise_sim(self, s1, s2):\n",
    "        sims = []\n",
    "        for t1 in s1:\n",
    "            for t2 in s2:\n",
    "                sims.append(self.similarity(t1, t2))\n",
    "        \n",
    "        if len(sims) == 0:\n",
    "            sims = [1.0]\n",
    "        \n",
    "        return np.array(sims)\n",
    "    \n",
    "    def fit(self, rows, *args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, rows):\n",
    "        out = []\n",
    "        for row in rows:\n",
    "            max_len = max(len(row['s1']), len(row['s2']))\n",
    "            sims = self.calc_pairwise_sim(row['s1'], row['s2'])\n",
    "            #sims = sorted(sims, reverse=True)[:min_len]\n",
    "            \n",
    "            out.append([\n",
    "                np.min(sims),\n",
    "                np.percentile(sims, 10)*max_len,\n",
    "                np.mean(sims)*max_len,\n",
    "                np.percentile(sims, 90)*max_len,\n",
    "                np.max(sims),\n",
    "                np.std(sims),\n",
    "            ])\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class GloVeSimilarity(Similarity):\n",
    "    ''' Calculates similarity as cosine distance between GloVe vectors '''\n",
    "    \n",
    "    def __init__(self, vec_path='glove.840B.300d.filtered.txt', dim=300):\n",
    "        self.vec_path = vec_path\n",
    "        self.dim = dim\n",
    "        self.w2v = self.load_vec()\n",
    "        \n",
    "    def load_vec(self):\n",
    "        ''' load word vectors from http://nlp.stanford.edu/data/glove.840B.300d.zip'''\n",
    "        w2v = {}\n",
    "\n",
    "        with open(self.vec_path, 'r') as f:\n",
    "            for line in f:\n",
    "                ls = line.strip().split(' ')\n",
    "                word, vec = ls[0], ls[1:]\n",
    "                w2v[word] = np.array(vec, dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "        return w2v\n",
    "    \n",
    "    def similarity(self, t1, t2):\n",
    "        v1 = self.w2v.get(t1.text, None)\n",
    "        v2 = self.w2v.get(t2.text, None)\n",
    "        \n",
    "        if v1 is not None and v2 is not None:\n",
    "            return cosine_similarity(v1, v2)[0][0]\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    \n",
    "class WordNetSimilarity(Similarity):\n",
    "    ''' Calculates similarity as Wu-Palmer metric '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.synsets = {}\n",
    "    \n",
    "    def get_synset(self, t):\n",
    "        t = t.lemma_\n",
    "        \n",
    "        if t in self.synsets:\n",
    "            return self.synsets[t]\n",
    "        else:\n",
    "            synsets = wordnet.synsets(t)\n",
    "            if len(synsets) > 0:\n",
    "                self.synsets[t] = synsets[0]\n",
    "                return synsets[0]\n",
    "            else:\n",
    "                return None\n",
    "    \n",
    "    def similarity(self, t1, t2):\n",
    "        synset1 = self.get_synset(t1)\n",
    "        synset2 = self.get_synset(t2)\n",
    "        \n",
    "        if synset1 is not None and synset2 is not None:\n",
    "            return wordnet.wup_similarity(synset1, synset2) or 0.0 \n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    \n",
    "class EntitySimilarity(Similarity):\n",
    "    ''' Calculates similarity as identity for named entities and 1.0 otherwise '''\n",
    "    \n",
    "    def similarity(self, t1, t2):\n",
    "        if t1.ent_type != 0 and t2.ent_type != 0:\n",
    "            return 1.0 if t1.lemma_ == t2.lemma_ else 0.0\n",
    "        else:\n",
    "            return 1.0\n",
    "        \n",
    "    \n",
    "class ProbSimilarity(Similarity):\n",
    "    ''' This metric doesn't have sense, but it worked the best '''\n",
    "    \n",
    "    def similarity(self, t1, t2):\n",
    "        return t1.prob*t2.prob\n",
    "        #return np.mean([t1.prob, t2.prob])\n",
    "        #return min(abs(t1.prob), abs(t2.prob))/max(abs(t1.prob), abs(t2.prob))\n",
    "\n",
    "class Logger(BaseEstimator, TransformerMixin):\n",
    "    ''' It prints given message '''\n",
    "\n",
    "    def __init__(self, msg):\n",
    "        self.msg = msg\n",
    "    \n",
    "    def fit(self, rows, *args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, rows):\n",
    "        print(time.time(), self.msg, len(rows))\n",
    "        return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Edits(BaseEstimator, TransformerMixin):\n",
    "    ''' Calculates Levenshtein distance with custom cost of substitutions '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lev = Levenshtein()\n",
    "        self.model = None\n",
    "        self.pipeline = make_pipeline(\n",
    "            make_union(\n",
    "                GloVeSimilarity(),\n",
    "                WordNetSimilarity(),\n",
    "                EntitySimilarity(),\n",
    "                ProbSimilarity(),\n",
    "            ),\n",
    "            make_pipeline(\n",
    "                make_union(\n",
    "                    EstimatorToTransform(\n",
    "                        RandomForestClassifier(\n",
    "                            n_estimators=100, \n",
    "                            max_depth=5,\n",
    "                        )),\n",
    "                    EstimatorToTransform(\n",
    "                        xgb.XGBClassifier(\n",
    "                            objective='binary:logistic', \n",
    "                        )),\n",
    "                    EstimatorToTransform(LogisticRegression()),\n",
    "                ),\n",
    "            ),\n",
    "            LogisticRegression(),\n",
    "        )\n",
    "        \n",
    "    def get_edits(self, s1, s2):\n",
    "        return self.lev.get_edits(s1, s2)\n",
    "        \n",
    "    def fit(self, rows, y, *args):\n",
    "        edits_X = []\n",
    "        edits_y = []\n",
    "        \n",
    "        # make edits\n",
    "        for row, current_y in zip(rows, y):\n",
    "            edits = self.get_edits(row['s1'], row['s2'])\n",
    "            row = {'s1':[], 's2':[]}\n",
    "            for edit in edits:\n",
    "                if edit['type'] != 'match':\n",
    "                    from_token = edit['from']\n",
    "                    to_token = edit['to']\n",
    "                    \n",
    "                    if from_token is not None:\n",
    "                        row['s1'].append(from_token)\n",
    "                        \n",
    "                    if to_token is not None:\n",
    "                        row['s2'].append(to_token)\n",
    "                else:\n",
    "                    if len(row['s1']) > 0 and len(row['s2']) > 0:\n",
    "                        edits_X.append(row)\n",
    "                        edits_y.append(current_y)\n",
    "                    row = {'s1':[], 's2':[]}\n",
    "        \n",
    "        # fit pipeline\n",
    "        self.model = self.pipeline.fit(edits_X, edits_y)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, rows):\n",
    "        \n",
    "        # make edits\n",
    "        i_edits = []\n",
    "        out_edits = []\n",
    "        for i, row in tqdm(enumerate(rows)):\n",
    "            edits = self.get_edits(row['s1'], row['s2'])\n",
    "            row_edits = {'s1':[], 's2':[]}\n",
    "            \n",
    "            for edit in edits:\n",
    "                if edit['type'] != 'match':\n",
    "                    from_token = edit['from']\n",
    "                    to_token = edit['to']\n",
    "                    \n",
    "                    if from_token is not None:\n",
    "                        row_edits['s1'].append(from_token)\n",
    "                        \n",
    "                    if to_token is not None:\n",
    "                        row_edits['s2'].append(to_token)\n",
    "                else:\n",
    "                    if len(row_edits['s1']) > 0 and len(row_edits['s2']) > 0:\n",
    "                        out_edits.append(row_edits)\n",
    "                        i_edits.append(i)  \n",
    "                    row_edits = {'s1':[], 's2':[]}\n",
    "        \n",
    "        # predict\n",
    "        pred_edits = self.model.predict_proba(out_edits)[:, 1]\n",
    "        i_preds = {}\n",
    "        for i, pred in zip(i_edits, pred_edits):\n",
    "            if i not in i_preds:\n",
    "                i_preds[i] = []\n",
    "            i_preds[i].append(pred)\n",
    "                \n",
    "        # aggregate\n",
    "        out = []\n",
    "        for i, _ in enumerate(rows):\n",
    "            preds = i_preds.get(i, [1.0])\n",
    "            out.append([\n",
    "                np.mean(preds),\n",
    "                np.min(preds),\n",
    "                np.max(preds),\n",
    "            ])\n",
    "        \n",
    "        return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 260 ms, total: 11.2 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipeline_freq = make_pipeline(\n",
    "    Logger('freq'),\n",
    "    make_union(\n",
    "        QuestionFreq(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "pipeline_common = make_pipeline(\n",
    "    Logger('common'),\n",
    "    make_union(\n",
    "        CommonTokens(),\n",
    "        CommonEnts(),\n",
    "        CommonNouns(),\n",
    "        CommonVerbs(),\n",
    "        CommonAds(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "pipeline_bow = make_pipeline(\n",
    "    Logger('bow'),\n",
    "    make_union(\n",
    "        make_pipeline(\n",
    "            ExtractCommonTokens(),\n",
    "            TfidfVectorizer(min_df=10),\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            ExtractDifferenceTokens(),\n",
    "            TfidfVectorizer(min_df=10),\n",
    "        ),\n",
    "    ),\n",
    "    EstimatorToTransform(LogisticRegression()),\n",
    ")\n",
    "\n",
    "pipeline_ensemble = make_pipeline(\n",
    "    make_union(\n",
    "        EstimatorToTransform(\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=100, \n",
    "                max_depth=5,\n",
    "            )),\n",
    "        EstimatorToTransform(\n",
    "            xgb.XGBClassifier(\n",
    "                objective='binary:logistic', \n",
    "            )),\n",
    "        EstimatorToTransform(LogisticRegression()),\n",
    "    ),\n",
    ")\n",
    "\n",
    "pipeline_edits = make_pipeline(\n",
    "    Logger('edits'),\n",
    "    Edits(),\n",
    ")\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    Logger('selector'),\n",
    "    PandasSelector(['question1', 'question2'], ['s1', 's2']),\n",
    "    \n",
    "    Logger('parser'),\n",
    "    Parser(['s1', 's2'], ['s1', 's2']),\n",
    "    \n",
    "    Logger('union'),\n",
    "    make_union(\n",
    "        pipeline_freq,\n",
    "        pipeline_common,\n",
    "        pipeline_bow,\n",
    "        pipeline_edits,\n",
    "    ),\n",
    "    Logger('train'),\n",
    "    pipeline_ensemble,\n",
    "    \n",
    "    Logger('ensemble'),\n",
    "    LogisticRegression(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read train data\n",
    "df = pd.read_csv('raw_data/train.csv').fillna('')\n",
    "df.set_index('id', inplace=True)\n",
    "\n",
    "y = np.array(df['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [00:00, 587.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1495958265.5940852 selector 404290\n",
      "1495958265.665987 parser 404290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404290it [11:41, 576.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1495958966.9207325 union 404290\n",
      "1495958966.9214745 freq 404290\n",
      "1495959049.480635 common 404290\n",
      "1495959109.4244938 bow 404290\n",
      "1495959142.5172174 edits 404290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404290it [01:16, 5275.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1495967456.0878403 train 404290\n",
      "1495967518.5555809 ensemble 404290\n",
      "CPU times: user 2h 37min 10s, sys: 23.1 s, total: 2h 37min 33s\n",
      "Wall time: 2h 34min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = pipeline.fit(df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('raw_data/test.csv').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1495967526.1804864 selector 2345796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:00, 433.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1495967526.5571375 parser 2345796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2345796it [1:11:31, 546.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1495971818.2335167 union 2345796\n",
      "1495971818.234361 freq 2345796\n",
      "1495971927.2793283 common 2345796\n",
      "1495972292.733938 bow 2345796"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "603it [00:00, 6025.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1495972456.6770084 edits 2345796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2345796it [06:54, 5653.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1496002145.03077 train 2345796\n",
      "1496002165.1504977 ensemble 2345796\n",
      "CPU times: user 9h 36min 46s, sys: 1min 51s, total: 9h 38min 37s\n",
      "Wall time: 9h 37min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred = model.predict_proba(df_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(OrderedDict({\n",
    "    'test_id': df_test['test_id'], \n",
    "    'is_duplicate': pred,\n",
    "})).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
