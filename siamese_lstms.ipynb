{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, LSTM, Embedding, Merge\n",
    "from keras.layers.merge import Concatenate\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_VOCABULARY = 1193514\n",
    "EMBEDDING_DIMENSION = 200\n",
    "OFFSET = 3\n",
    "OOV_TOKEN = 0  # out of vocabulary\n",
    "EOS_TOKEN = 1  # end of sentence\n",
    "PAD_TOKEN = 2  #padding to max sentence length\n",
    "MAX_SENTENCE_LENGTH = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_A = []\n",
    "questions_B = []\n",
    "labels = []\n",
    "for i, row in train_dataframe.iterrows():\n",
    "    question_A = str(row['question1'])\n",
    "    question_B = str(row['question2'])\n",
    "    if len(question_A) < MAX_SENTENCE_LENGTH or len(question_B) < MAX_SENTENCE_LENGTH:\n",
    "        continue\n",
    "    questions_A.append(question_A)\n",
    "    questions_B.append(question_B)\n",
    "    labels.append(row['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def clean_questions(questions):\n",
    "#     punctuation_chars = set(punctuation)\n",
    "#     punctuation_chars.remove('\"')\n",
    "#     punctuation = set(punctuation)\n",
    "#     questions = [q.split() for q in questions]\n",
    "#     return questions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.zeros(shape=(EMBEDDING_VOCABULARY+OFFSET, EMBEDDING_DIMENSION))\n",
    "word2idx = {}\n",
    "i = OFFSET\n",
    "for line in open('glove.twitter.27B.200d.txt', 'r'):\n",
    "    items = line.replace('\\r','').replace('\\n','').split(' ')\n",
    "    if len(items) < 10: \n",
    "        continue\n",
    "    word = items[0]\n",
    "    word_vectors[i, :] = np.array([float(j) for j in items[1:]])\n",
    "    word2idx[word] = i\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = max(len(q) for q in (questions_A + questions_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_to_word_indices(questions):\n",
    "    result = []\n",
    "    for question in questions:\n",
    "        result.append([word2idx[word] if word in word2idx else OOV_TOKEN for word in question] + [EOS_TOKEN])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_A = cast_to_word_indices(questions_A)\n",
    "questions_B = cast_to_word_indices(questions_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions_B[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_A = pad_sequences(questions_A, maxlen=MAX_SENTENCE_LENGTH, value=PAD_TOKEN)\n",
    "questions_B = pad_sequences(questions_B, maxlen=MAX_SENTENCE_LENGTH, value=PAD_TOKEN)\n",
    "labels = to_categorical(labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer \"embedding_5\" with a  weight list of length 1193517, but the layer was expecting 1 weights. Provided weights: [[ 0.        0.        0.       ...,  0.        0....",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-a64127b25da2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m )(input_A)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0msentence_representation_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdroput_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_representation_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tomek/.virtualenvs/dl3/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0;31m# Load weights that were specified at layer instantiation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tomek/.virtualenvs/dl3/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1146\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              \u001b[0;34m' weights. Provided weights: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                              str(weights)[:50] + '...')\n\u001b[0m\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You called `set_weights(weights)` on layer \"embedding_5\" with a  weight list of length 1193517, but the layer was expecting 1 weights. Provided weights: [[ 0.        0.        0.       ...,  0.        0...."
     ]
    }
   ],
   "source": [
    "shared_lstm = LSTM(64, return_sequences=False, go_backwards=True)\n",
    "\n",
    "\n",
    "input_A = Input(shape=(MAX_SENTENCE_LENGTH,))\n",
    "embeddings_A = Embedding(\n",
    "    input_dim=EMBEDDING_VOCABULARY + OFFSET, \n",
    "    output_dim=EMBEDDING_DIMENSION, \n",
    "    input_length=MAX_SENTENCE_LENGTH,\n",
    "    weights=word_vectors,\n",
    "    trainable=True\n",
    ")(input_A)\n",
    "sentence_representation_A = shared_lstm(embeddings_A)\n",
    "droput_A = Dropout(0.5)(sentence_representation_A)\n",
    "\n",
    "\n",
    "input_B = Input(shape=(MAX_SENTENCE_LENGTH,))\n",
    "embeddings_B = Embedding(\n",
    "    input_dim=EMBEDDING_VOCABULARY + OFFSET, \n",
    "    output_dim=EMBEDDING_DIMENSION, \n",
    "    input_length=MAX_SENTENCE_LENGTH,\n",
    "    weights=word_vectors,\n",
    "    trainable=True\n",
    ")(input_B)\n",
    "sentence_representation_B = shared_lstm(embeddings_B)\n",
    "droput_B = Dropout(0.5)(sentence_representation_B)\n",
    "\n",
    "merged_model = Concatenate(axis=-1)(dropout_A, dropout_B)\n",
    "predictions = Dense(2, activation='softmax')(merged_model)\n",
    "\n",
    "Model(inputs=[input_A, input_B], outputs=predictions)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.decribe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193517, 200)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
